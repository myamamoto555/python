A	Okay .
A	Does anyone want to see uh Steve's feedback from the specification ?
D	I I dry-read it the last time ..
A	Right .
A	Not really , um just what he's talking about , like duplication of effort and
A	Like duplication of effort and stuff , and umyeah , he was saying that we should maybe uh think about having a prototype for week six , which is next week . Yeah .
D	Next week .
A	So we should probably prioritize our packages .
A	Mm .
B	Yeah .
A	Yeah .
B	Yeah , I mean if we just want to have um some data for the user face , could even be random data . Uh mm m
D	Yeah .
B	Yeah , I'm
D	Yeah .
A	Yeah .
A	Hmm . Has has anyone actually looked at the Java code for th, huh ? Hmm .
D	No .
A	Yeah , I think so .
C	The basic word importance is off-line as well . The combined measure might not be if we want to wait what the user has typed in into the search .
D	Uh mine's gonna be mostly using the off-line . But the actual stuff it's doing will be on-line . But it won't be very um processor intensive or memory intensive , I don't think .
A	Yeah , I I don't know about the search functionality , that might be online . Depends how it's gonna work .
B	Hmm .
A	Yeah .
D	Don't think so .
D	Yeah .
D	Are we still gonna go for dumping it into a database ? Are we still gonna dump it into a database ?
D	'Cause if we are , I reckon we should all read our classes out of the database . It'll be so much easier .
D	Well if we're gonna dump the part of it into a database anyway , we might as well dump all the fields we want into the database , calculate everything from there . Then we don't even have to worry that much about the underlying X_M_L_ representation . We can just query it .
D	Well if we're gonna do that , we should try and store everything in in an X_M_L_ format as well .
D	Yeah .
B	Yes .
D	Yeah .
A	Mm-hmm .
B	Hmm yes .
B	Hmm .
A	Yeah , that makes sense .
B	I'm not so sure .
D	Well we don't even need to do that , 'cause if we got our information density calculated off-line , so all we do is treat the whole lot as one massive document . I mean they'llit's not gonna be so big that we can't load in a information density for every utterance . And we can just summarise based on that .
B	I I thought we would just have like um one big summary um with all the uh different importance levels um displayed on it . And depending on what our um zoom level is , we just display a part of it .
B	And we would have one very big thing off-line . And from that we would just select what we are displaying .
B	Yes . So for example you would um give a high value to those um sequences you want to display in the meeting series summary . And you just cut off .
B	That was what I sh I thought , yeah . I thought .
D	I think you can do it on-line .
A	Hmm .
D	I don't think there's really much point in doing like that when it's just gonna feed off in the end the information density measure basically . And that's all calculated off-line . So what you're really doing is sorting a list , is the p computationally hard part of it .
D	Well like the ideas we're calculating are information density all off-line first for every utterance in the whole corpus , right ? So what you do is you say if you're looking at a series of meetings , you just say well our whole document comprises of all these stuck together . And then all you have to do is sort them by j information density . Like maybe weighted with the search terms , and then extract them . I don't think it's too slow to do on-line , to be honest .
D	Is that
B	But I think the m difference might be that we wantjust want to have um the words . And that's not so much what he meant with not possibly loading everything was that you m um load all the uh annotation stuff , all the sound files , all
D	Yeah .
A	Hmm .
D	Well , on the utterance level I was thinking . So the utterances with the highest like mean information density .
B	I
D	Well the trouble with doing it on the word level is if you want the audio to synch up , you've got no way of getting in and extracting just that word . I mean it's impossible .
D	For every single word ? Oh , okay .
C	Yeah .
D	Yeah .
D	I don't think thatwill do it . We'll have to buffer it .
B	UmI r II'm getting quite lost um at the moment because um w what's um our difference between the um se umuh the importance measure and the skimming ? I mean , do we do both or is it the same thing ?
D	Well the skimming's gonna use the importance .
B	Okay .
D	But like at first it's just gonna be I_D_F_ .
B	Sobut when when we talk about summaries you talk about thisuh abo about skimming and not about
D	Well mostly skimming , yeah .
B	Yeah .
B	Yeah right , isn't that the skimming ? Isn't that the skimming ?
B	Yeah , but it use the same data .
D	Yeah .
D	Well the nice thing about that is it will automatically be in sentences . Well more or less . So it will make more sense , and if you getjust extract words .
C	I'm not quiteso what itdid you want to do it , i you just wanted to assign
C	Uh I thought about words .
C	Mm .
B	Yeah .
B	A 
D	Yeah .
B	And , yeah , I think we also thought about combining that measure with um the measures I get from um s uh hot-spots and so on . So that would also be on utterance level , I think . I think .
C	Mm okay .
D	I see it .
D	But it'll need to be calculated at word level though because otherwise there won't be enough occurrences of the terms to make any meaningful sense .
D	Yeah .
D	Yeah , I reckon you can just mean it over the sentence .
C	Yeah , but how about those words which don't carry any meaning at all , the um and uhs and something like that .
C	Because if we if we average average over over a whole utterance all the words , and there are quite unimportant words in there , but quite important words as well , I think we should just disregard the the
D	I think we should filter them .
D	Maybe we should have like um a cut-off . So ita w word only gets a value if it's above a certain threshold . So anything that has less than say nought point five importance gets assigned to zero .
C	Okay .
C	Alright .
D	Yeah , that's the other th
D	Yeah .
D	I think we'll have to buffer the audio . But I don't think it will be very hard . I think it would be like an hour or two's work .
D	Like just build an another f wave file essentially .
A	Yeah , you just concatenate them together .
D	Yeah , I mean I bet there would be packages
D	In memory , yeah . So just like unp there's bound to be like a media wave object or something like that . And just build one in memory .
D	I don't know . I have no idea . But it must have like classes for dealing with files . And if it has classes for concatenating files , you can do it in memory . So
D	Well what I think I might try and build is basically a class that you just feed it a linked list of um different wave-forms , and it will just string them all together withmaybe , I don't know , tenth of a second silence in between each one or something like that .
D	Normalise it , yeah .
D	Oh yeah , yeah , we'll need that .
B	Yes , sure .
D	We also really wanna be able to search by who's speaking as well .
D	It doesn't matter , 'cause all the calculation's done off-line .
A	Hmm .
B	Yes .
A	Yeah .
A	It just means it loads on demand . It only loads when it needs a particular type of file . Like when it's being accessed .
A	Yeah , I think that's the idea , it just loads the particular ones it needs .
A	But if you were doing a search over the whole corpus you'd have to load them all .
B	Yes , right .
A	Hmm .
B	Oops , it does . So I define baseline and what it loads ? For example it loads all the utterances and so on , but it doesn't load um the discourse acts and for example not theand what'swhat elsethere ? Not the summaries . It only loads those on demand .
B	Y you mean that you um basically split up th the big thing into um different summaries . For example that you have a very um top-level um summary and a separate file for for each level .
D	That's easy . You just like create a new X_M_L_ document in memory .
B	Mm-hmm .
A	Mm .
D	I don't think it's really that much of a problem because if it's too big , what we can do is justwell all the off-line stuff doesn't really matter . And all we can do is just process a bit at a time . Like for summarisation , say we wanted a hundred utterances in the summary , just look at the meeting , take the top one hundred utterances in each other meeting . If it scores higher than the ones already in the summary so far , just replace them . And then you only have to process one meeting at a time .
D	Okay , so maybe we should build a b store a mean measure for the segments and meetings as well ?
D	And speaker .Speaker and um topic segmenting we'll need as well .
D	Yeah .
D	Well yeah , and then it'll f preserve the order when it's displayed the
D	Yeah .
D	Yeah .
D	Yeah , I think so .
A	Hmm .
A	Yeah , we do not want it into develop a little tree display as well for multiple results .
A	Yeah , but that'd be quite easy to do .
A	You just need to find the time stamp .
A	Yeah .
A	Yeah .
D	So we should basically make our own X_M_L_ document in memory
D	that everyone's um
D	module changes that , rather than the underlying data . And then have that X_M_L_uh NITE X_M_L_ document tied to the interface .
D	Well , you can make it in a file if you want .
D	Mm-hmm .
C	Yeah .
B	Yes .
C	But there is no I_D_ for an utterance I think . It's just for individual words . So how do we do that then ? Wefor utterances as well .
B	N Uh no no , it's f for
B	No , you're right . Yeah . It's for
C	I think it's just for one
C	word . So we have to
A	Yeah , I think I think those segments for each utterance are split up .
C	Yeah .
A	Think so .
A	Yeah , I'm pretty sure it's already there . Pretty sure that's already there . The the utterances are numbered .
B	Um
C	Uh I'm not quite sure , I have only seen that the uh the individual words have got an I_D_ .
C	Yeah . You always could have a look at the time stamps and then take the ones that uh belong together to form an utterance .
B	No , I I think we would just take the segments that are alreadythat werYeah , there's um this segments file . Um you know , the X_M_L_ segments .
C	Yeah , if they are already , there'sit's easy but it would be possible .
C	Uh yeah . Okay .
A	Hmm .
B	Oh .
B	That I don't know .
A	Yeah , I think so .
D	They are utterances , aren't they ? The segments are utterances , aren't they ? Yeah .
A	Ye that's the impression I get , yeah .
B	Yeah , that's um
D	Alright , okay .
B	Mm-hmm .
A	Oh .
B	There there are time stamps um for , well , segments um and for th um segments is for example when when you look at the data , what is displayed in one line .
D	Well , that's easy .
A	Hmm .
B	Whatwhen when you look at it in thehmm ?
A	Ye 
A	Mm .
D	Well it's close enough , isn't it ? It may not be exact every time , but it's a so sort of size we're looking for .
B	I think so . Isn't
A	Yeah , uh
B	Um for ex um I I compared it with what I did for the pause um duration extraction . Um and basically it's uh words that are uttered in a sequence without pauses . But sometimes um however there are um short pauses in it and they're indicated by square brackets pause or something in the data .
A	Right .
B	Um someti uh but uh the annotators decided what was one segment and what wasn't .
B	I think so .
D	Yeah , yeah .
A	Okay .
A	Topics , yeah .
A	Yeah , I think that's the right one .
B	Yeah , but um I think for some annotations um an uttera ca utterance can have several um types . For example for the dialogue acts and so on .
A	Hmm .
B	Okay . Yeah , that should be for
A	Hmm .
B	Yeah . Should be , yeah .
A	Mm-hmm .
D	Yeah .
D	But why don't we just write it as a new X_M_L_ file ? Can NITE handle just loading arbitrary uh new like attributes and stuff ?
D	I mean , I would have thought they'd make it able to .
D	Yeah .
D	So why do we need to have two X_M_L_ trees in memory at once ?
D	The other thing is that would mean we'd be using their parser as well , which means we wouldn't have to parse anything , which be quite nice . 'Cause their parser is probably much faster than anything we've come up with anyway .
A	Mm .
D	Yeah , I mean we can process it in chunks if it gets too big basically . We can just process it all in chunks if it gets too big to load it into memory .
A	Hmm .
D	I think we probably want to storSorry . I think we probably want to store um a hierarchical information density as well . So like an informan mation density score for each meeting and each topic segment . 'Cause otherwise we'd be recalculating the same thing over and over and over again .
A	Yeah , that'd be much more efficient to do that . Yeah .
D	Yeah .
D	And that will obviously make it much easier to display .
D	Well it may not for the whole meeting , but like
D	Yeah , exactly .
D	Yeah .
D	Well , we can start off
D	like that . Well I was gonna start offI've v got sort of half-way through implementing one that does just I_D_F_ .
D	And then just I can change that to work on whatever .
D	Yeah .
A	Hmm .
D	And it should be weighted by stuff like the hot spots and um the key-words in the search and stuff like that .
A	Hmm .
D	Did he not say something about named entities ? So I thought he said there wasn't very many .
D	Yeah .
C	You s uh you said you arecurrently in uh implementing the idea . What exactly are you computing ?
D	Yeah . It's not T_F_I_D_F_ , it's just inverse document frequency . 'Cause it's really easy to do basically .There's just like for a baseline really .
C	Okay .
C	Okay .
C	Mm-hmm .
A	Yeah , you're able to do that in Java , yeah ?
D	Well , I'm half-way through . It's not working yet , but it will do .
A	Yeah .
D	Um yeah . And then averaging it over the utterances .
D	But it's not like um related to the corpus at all . It's just working on an arbitrary text file at the moment .
D	No .
A	Huh .
D	It would be useful to know how everyone's gonna store their things though .
A	Hmm .
D	Yeah .
D	Yeah . Well I've got like a few hours free . Like after this .
A	Yeah , I've had a b I've had a look at the the topic segments , how it's stored . And thenyeah , th those are few per meeting , and itum well , it gives a time stamp and inside each one there's uh the actual like utterance segments .
A	And the list of them that occurred . And they're all numbered .
A	Um so that's where that's stored .
A	Yeah , so I guess um if I'm gonna be segmenting it with a L_C_ seg then that's like same format I'd want to um put it back out in so it'd be equivalent .
A	Well , like the integration . What do you mean , integration ?
A	Hmm .
A	I don't know . I don't think anyone's been allocated to do that yet .
A	Yeah , yeah .
A	Yeah , definitely .
A	Hmm , yeah .
D	It's the most boring task .
C	Mm-hmm .
A	Yeah , it c could be difficult , yeah .
D	Yeah .
A	Yeah .
A	Well I guess the important thing is to get the crucial m modules built .
D	Or at least umsimple versions of them .
A	Ye yeah .
A	Yeah , and then
A	Yeah , and then we'll maybe have to prioritize somebody into just integrating it .
D	So maybe we should try doing something really simple , like just displaying a whole meeting .
D	And like just being able to scroll through it or something like that .
D	Yeah .
D	Are you free after this ?
D	How about Friday then . 'Cause I'm off all Friday .
D	Uh Wednesday I've got a nine 'til twelve .
D	Yeah , nothing in the afternoon . I've got nothing in the afternoon . So
D	Okay . So you ha yeah .
D	Where about , just in Appleton Tower ?
D	Uh I'll be in um the Appleton Tower anyway .
D	Um well I'll be there from twelve . I've got some other stuff that needs done on Matlab , so if you're not there at twelve , I can just work on that . So
D	Yeah .
A	Mm-hmm .
A	Yeah , I think so .
C	Yeah , I w I wI would need the raw text pretty soon because I have to find out um how I have to put the segments into bins . And thenyeah .
A	Uh yeah .
C	No ,that's not necessary .
A	Hmm . Yeah , yeah . Jasmine , I thought you just said that you'd uh looked at extracting the text .
D	Why w
C	Yes , I did . But um I've only just got the notes . I have to still have uh to order everything by the time and
A	Yeah . So you you said you did it in Python , yeah ?
C	Yeah , I think it's quite easy after the
A	Yeah , did you use uh b the X_L_uh X_M_L_ parser in Python ? Right . Yeah , sounds pretty good . So um 'cause , yeah , I was having a look in ita look at it as well and I noticed theum the speakers are all in that separate file ? So did did you have to combine them all and and then re-order them ?
C	Yeah . Yeah .
C	So uh
C	Mm-hmm .
C	Yeah , b I uh w that's what I wasuh thought . That you just combine them and then order the time stamps accordingly .
A	Yeah .
A	Ye yeah , c
A	Right . Yeah , so that's approacum well , I was going to do . So yeah , we may as well collaborate .
C	Okay . Um what I found out wasthat there are quite a lot of things without without s time stamps in the beginning .
A	In the word files ?
C	Yeah , and uh X_M_L_ files . Yeah , that's just an I_D_ or something . I don't know . Just numbers .
B	Yes , but that's
B	Yeah , everything that's a word has a sti time stamp .
C	Yes , but what are the other things that's uh some kind of number ? F maybe the file number or something that is in the beginning . What is that ?
A	I'm not sure Iwhat you mean .
C	Do you know?
C	Um I think there are quite a lot of numbers in the beginning where n there is no time stamp for the numbers .
C	It's
C	Think they say um quite a lot of numbers and before that , uh um there's this number . Was it
C	Yeah , there i are numbersin the um the W_ tag , but there are no time stamps .
C	Yeah .
B	That's at the end . That's at the end , I think , her time .
C	Yeah , in the beginning as well sometimes , I think . At least I saw some.
A	Oh right .
B	Yeah , maybe .
B	Didn't have a look at our meetings .
C	Yeah .
C	Yeah .
A	Hmm .
B	Uh I I think it wouldn't as it occursI mean it would beit occurs in every meeting . S
B	And I think it even has uh its own annotation , like digits or something . So that should be really easy to cut out .
B	Yeah .I'm sure .
C	But what itis it actually that numbers ?
B	Ah it's just to test the system , I think .
C	Okay , so but there are no time stamps annotated to that . It's it's quite strange .
B	So
B	Mm they have to read numbers from
B	Uh I didn't have a look at that . S
A	Hmm .
C	And also um there are different um combinations of letters . B_R_E_ and something like that . Is it everything orderedare the time stamps global or uh are they local at any point ?
A	Mm I thought they were local to th a particular meeting .
B	They
C	Okay .
D	Yeah . I'm just building a dictionary .
D	Oh , mine's just gonna use the um hash map one in um Java . 'Cause I'm only gonna do it on small documents . It's just like bef until the information density is up and running . Just something to getgive me something to work with .
D	So it's only gonna use quite small documents , you see , to start with .
C	Yeah , it's Rainbow . It's umI think it's just the dictionary in the first place . But
C	Um no , I have to bin it up and so I will only have counts for each each bin or something .
D	Why does it need to be classified into like different segments ?
C	It's because um Rainbow is a text classification system . And I thinkit's not possibleto have just one class . That's the problem .
D	Can we just fill a second class with junk that we don't care about ?
C	Maybe we could
D	Like , I don't know , copies of Shakespeare or something .
C	Yeah sure , yousure , we could do that , but I don't that makes sense .
D	'Cause if what we're looking for is the um frequency statistics , I don't see how that would be changed by the classification .
C	If we need just frequencies , maybe we should just calculate them by using Perl or something . I don't know .
D	Ithe
D	Well there maybe another tool available ?
C	Yeah , it's quite easy to just count and s or sort them bum frequency .
D	Yeah .
C	Just using a Perl script .
C	Is it too big ?
C	Yeah .
C	Hmm .
C	I don't know how youhow many terms you can handle in Perl .
C	Mm yeah .
D	Um I can't remember who's got it . Might be WordNet . But one of these big corpuses has a list of stop words that you can download and they're just basically lists of really uninteresting boring words that we could filter out before we do that .
D	It's like that'sone the papers I read , that's um one things they did right at the beginning is they've got this big s stop-list and they just ignore all of those throughout the experiment .
D	Yeah , Iit would be useful for me as well .
D	Ituh I think that'd be useful for me as well . Yeah .
D	Yeah .
D	Well all you really wanna do is look into getting some sub-set of the ICSI corpus off the DICE machines . 'Cause I hate working on DICE . It's awful .
D	Like so I can use my home machine .
D	hahas a C_D_ burner though .has a C_D_ burner .
D	Yeah . The right-hand corner , far right .
D	Yeah .
B	Mm-hmm .
D	How big is it without um
D	the WAV files and stuff ?
D	'Cause I could just say at um going over S_C_P_ one night and just leave it going all night if I had to .
D	It'syeah , I mean the wave data are obviously not gonna get off there completely .
D	Really ? Oh right ?
D	I'll see if I can S_C_P_ it , I suppose .
D	I've got a Linux box and a Windows box . So
D	Broad-band .
D	Put it on to C_D_ . I canif I get down I can put to C_D_ .
D	Yeah .
D	I'm not sure if there's enough space . Ishow much do we get ?
D	Really ? Okay .
D	Yeah , but I can do it from that session , can't I ? You can compress it from a remote session and S_C_P_ it from the same session ?
D	Do you think ?
D	Yeah . Oh no no , I was thinking of SSHing just into some machine and then just SCPing it from there .
D	Yeah . I mean it has to go through the gateway . But
D	Can you not do that ?
D	Mm , I see .
B	Uh th yeah .
D	Yeah .
B	'Kay . Um I just um wondered , so who's uh then doing um the frequencies on on the words , because I'mI think I will also umI could also make use of it um for the agreement and disagreement thing . Because I umIin my outline I talked about um using the um discourse acts first , and um then in the chunks of text I found looking for word patterns and so on . So um I would for example need the um most freq um frequent words . So if you cut off all that , I'd won't be useor
B	Yeah , I Ibut I need it for my chunks then . I would
B	You know ?
B	Yeah , but I'duh I would like to look at the frequency of words in myum in the regions of text I found out to be interesting . So I wouldn't need it . It it would have to be re-calculated only for my segments .
D	So you could just
D	But th first , uh how big are the chunks ?
B	Huh ?
D	How big are the chunks you're looking at ?
B	Uhuhmm . I think it would be , you know , l as as big atas the hot-spot annotation things . That's quite small , yeah , that's some utterances .
D	So quite small then . So you could just umyou could use just the same thing we used to build the big dictionary . You just do that on-line 'cause that won't take long to build a little dictionary that big , will it . I mean just use the same tool that we use . Yeah .
A	Hmm .
B	Yes . Yeah , yeah . So I would probably just concatenate all my um text chunks and then let's say m I will run over it .
D	Yeah .
B	Yes .
C	UhI can get all the raw text , but it has to be ordered still . So
D	It doesn't need ordered , no .
C	No , it isn't .
D	Um well that's the t are you using T_F_I_D_F_ for the information density ?
C	Um it's inwhat is implemented in Rainbow is information gain , and I'm not quite sure how they calculate that .
D	Alright , okay .
D	Like 'cause frequency would be useful , I think . But um depending on the context , the size , and what we consider a document in the sense of calculating T_F_I_D_F_ is gonna change .
C	Yeah .
D	Which might need thinking about .
C	Uh that's what Rainbow does . I think you j can just get probabilities for acertain wordsfor each document .
C	Certain
C	Um we would have to look at that .
C	Mm-hmm .
C	Oh .
D	I think it would be useful , yeah .
D	Wellyou need the raw frequency as well . But um
D	you also need how many times things occur within each document .
D	And um what we consider a document's gonna depend on our context , I think . 'Cause if we're looking at the whole lot of meetings , we'll consider each meeting a document in sort of terms of this algorithm . And if we're viewing like say just a small topic segment you might look at even each utterance as a small document .
D	Yeah , but the thing is um
D	It's gonna need some th th thought of how we
D	Actually maybe it doesn't actually matter . Maybe if you just do it once at the highest level , it it will be fine . But I was just thinking it might be difficult to calculate the T_F_I_D_F_ off-line for all the different levels we might want . 'Cause if we're gonna allow disjoint segments for example , then how are we gonna know what's gonna be in context at any given time ? But I suppose if you just did it globally , treating a meeting as a document , it'd probably still bework out fine , because you'd only be comparing to ones within the context .
D	Uh I don't know , I thoughtwere you gonna use that in the end ?
D	The information density .
D	Oh sorry , that's what I mean . Like umyeah , for each word or whatever , but across the whole lot is what I mean by highest level . Like across the whole corpus .
D	Yeah , but you'd probably look at each meeting as a document .
D	Mm possibly .
C	Yeah , that's what I thought as well , that youthat probably the the topic segment level is the most um informative for the words .
D	Are they big enough to get anything meaningful out of ?
C	Yeah , that's the problem . I don't know .
D	Well yeah , that is notit's not an issue . You just concatenate an X_M_L_ file together . but we still want to have like a notion of meetings for the user .
B	Yes , definitely .
D	Yeah , sure . Yeah , you justlike whatever you want to look at , you just jam together into an X_M_L_ file and that's your meeting , even though bits of it may come from all over the place or whatever . I mean I don't see why that's really a big problem .
D	So basically what you're saying is you can take an arbitrary amount of data and process it with the same algorithm . It doesn't matter conceptually what that data is . It could be a meeting . it could be two utterances . it could be a meeting plus half a meeting from somewhere else .
D	I don't think it's very difficult though . I mean what you do is you just build an X_M_L_ file , and if you want it to get down to the utterances , you'd go to the leaves . And then if you wanted the next level up , you'd go to the parents of those and like just go from like the leaves inwards towards the branch to build up things like umyou know , when you click on a segment , it's gonna have like words or whatever that are important .
D	As long as like the algorithms are designed um with it in mind , I don't think it's a very big problem .
D	Well like say you had umlike say for a meeting , right , you've got like uh say a hierarchy that looks quite big , like this .
D	And like the utterances come off of here maybe .
D	Then when whatever your algorithm is doing , as long as when you're working with utterances , you go for all the leaves , like then if you need something next up , so like a topic segment , you'd go to here . But if you were looking at say this one , so only went like this .
D	Right , so youit's same , you'd start with the leaves , and you go oh , I want a topic segment . So I go one layer up .
D	See , and then if you're working with just a topic segment in there , it's the only thing you have to worry about . And like each time you want a higher level , you just need to go up the tree . And as long as your algorithm respects that , then we can just
D	process any arbitrary X_M_L_
D	file with whatever hierarchical structure we want .
D	A meeting , say , and that would be a topic segment .
D	So I think as long as you build an algorithm that respects whatever structure's in the file , rather than imposing its own structure
D	Well no , it doesn't have to be .
D	But I mean it could be as many nodes as you want . Like this one could be deeper maybe ,
D	say . So then you'd start with all your utterances here , and when you go up to get topic segments , you go to here here here here here here here .
D	That might be a bit confusing though 'cause you have things on different levels .
A	Mm is there anything else we should discuss ?
D	Well Wednesday . Yeah .
D	Yeah .
D	So we'll see if we can get like a mini-browser just displays two things synched togetherof some kind . Yeah . Yeah .
A	Yeah , should we not have like a group directory or something where we can put all our code in and that kinda thing ?
D	It'd be useful . I don't know who you see about that though .
A	Hmm .
A	I've gotten mm hardly any
C	Mm-hmm .
D	I d have no idea .
D	I've probably got a reasonable amount because um everything on my DICE account can actually be deleted 'cause I store it all at home as well .
A	Hmm .
D	Is that guaranteed to stay , the
A	Yeah , we can ask Steve if um we can get space .
D	Maybe you should send a support form .
A	Yeah , uh we could do that .
D	Just say we want some web space .
D	Listen to .
A	Yeah , I'm sure he had to deal with that last year .
D	Yeah . 'Cause that'd be really useful is if we had a big directory . Especially for transferring stuff .
D	Having said that , are we allowed to take a copy of the ICSI corpus ? Something we should probably ask before we do it .
D	.
D	Okay .
D	Okay .
D	No , me neither .
B	Yeah , right .
B	Ye
B	M 
C	So shall we sit together tomorrow then as well ? UhOkay .
A	Yeah .
B	Um Jasmine , uh um what is um the text you're extracting uh looking like then at the end ?
C	Um , yeah , w would it be best ? At the moment it's it's just lines o
B	Because umI I think it's actually very similar to what I did for my um speaker um uh extraction and I think I would ch perhaps have to change two lines of codes to get you um for each meeting a file that says fr from um this millisecond to this millisecond there was this sequence of words .
C	Mm-hmm .
B	And so on .
C	Um
B	So that's just changing two lines of code . And it would give you that . So
C	Okay . So um you'ddo you extract the words , the raw text , as well ? Uh
B	Um yeah . So far I extracted um the dura durations . But it's from the words file . So I could just um contatenateconcatenate um the words instead of the durations , and it shouldI mean
C	Okay .
C	Mm-hmm .
C	Print out .
B	Should be very
B	straight-forward .
B	I can try to do it and send it to you . Pe and you have a look at it , will it make sense for what you want .
C	Okay . Okay , that
C	Okay .
C	So have we already extracted from all the files ?
B	Yeah , uh p I mean itI just let it run over all the files . So
C	Yeah .
C	Did you also order
B	Yes . I just ordered .
C	Mm-hmm .
B	Uh I ordered according to the um starting times of the utterances .
C	Hmm .
B	What do you mean by diffe
A	Hmm .
B	Yeah , I mean t I I have onewhat I give you would be one file for each meeting .
B	Yeah , not for each meeting series . I didn't do that yet .
B	Yeah , one group , yeah .
C	Hmm .
D	Might be funny to see what is summarised the whole corpus as anyway .I think it'd be very useful . But
B	Yeah , I mean there's one series that has just one meeting 
A	That sounds good .
B	Yes . Um theyou youthe data is of the form you have um three identification letter . So B_E_D_ or B_B_D_ or something , and that's always the same group . And then after that there's um a number like O_O_ one , O_O_ two . So , it's a
B	Yeah , but that's that's really quite easy to see because they're named .
A	Hmm .
B	Yes . But I I mean as um the startuh start times um start for each meeting at zero , you could just probably just um add the um final second time to the next meeting and so on and just put it all together . But then we would have to change um the information about whoon which channel it was set , um toby which person it was set . And that is actually stored in another X_M_L_ document .
A	Hmm .
A	Yeah , that's what I'm gonna need .
B	Yeah , I w would then just not print out the um start and end times .
A	Yes .
A	Yeah , it's just mo changing it a bit .
C	Okay .
B	No , it's for every single word .
D	We can just change the code .
B	Or for every single utterance . Yeah , that depends on what you want .
A	Yeah .
B	Yeah , but I do it with Perl , it's just string manipulation . So I wouldI mean I would just
B	Sure .
B	No , I didn't do a sea no .
A	No , butuh that's what M_L_C_ seg does . It it marks the end of each segment .
A	Yeah .
A	Yeah .
B	And you would want that all in one file for all the corpus ? Or
B	For the series .
B	Yeah , I can directly put it intouh just like
B	So uh only words um per meeting series .
A	Oh .
A	Yeah .
A	Yeah , for me it's better if they're by meeting .
B	Uh-huh .
B	Yes .
B	Yeah , they will justI will just takeI would uh take over the names they have anyway .
B	Yeah , yeah .
B	Yeah , one series has the um same three starting letters . So
B	So only words and words and times . And you
C	Uh I don't need the times , I just need the words . But um
B	Yeah , you want it ordered . Okay .Okay , anybod
C	Yeah , in the right order . Yes .
B	Um ord base dot times .
B	Yeah , and do you want
B	Yeah , sometimes they're contained in one another . So
C	Yeah , that doesn't matter too much , I think .
B	Just after th mm-hmm . 'Kay .
B	Ordered . Only words .
A	Then that'll be really easy to do once they've got the raw text . It's just a case of running the script .
A	Yeah , I mean hopefully this week .
B	Um and I think um for all the corpus , it's just from I know from other times , it's um nine megami byte to haveI mean should beshould be similar to have the words . Should be .
D	Is that it ? That's quite good .
B	Na um all the words together um for all the meetings .
A	Alright .
B	That's what I'm guessing that's , you know , um what Ibecause nine mega-byte is what I got forwhen I said for every um utterance , this isgoes from there to there and
B	takes takesseconds .
A	And we could
B	Oh . Yeah , I mean I'm it doing it for all of it . Doesn't matter .
D	Yeah .
B	Yeah , I mean I hope it will be the same for the words . It's just what I
B	I
D	I could just use it with the frequency , I think , until the information density thing's finished . That would be really useful .
B	Mm-hmm . Mm . So so um I will probably send um just one file of the first meeting um to all those who need it so that you can have a look whether that's what you want .
C	Hmm .
C	Mm-hmm .
B	Yeah , I mean if it's just for one meeting , it's really not too big .
B	Yeah .
C	How long would it take to make the frequency counts with a Java hash table ?
C	Yeah .
C	No , how long you would have to program something .
C	Okay .
C	Mm .
C	Because it's quite easy in Perl as well , it's just a line of code for counting all the words andyeah , it's it's by hashes .
C	Yeah .
C	Yeah .
C	'Kay .
D	If you're doing it in Java , could you um serialize the output as well as writing it to a file ? If you're doing it in Java , could you serialize the um dictionary , yeah , as well as writing it to a file ?
D	It's really easy .
D	I don't see why it'd be any more massive than the file .
D	Yeah .
D	It just saves you parsing the um
D	file representation of it . And now'cause I would be using it in Java anyway . So I'd just be building the data structure again .
D	Yeah , but it seems like a bit silly to be parsing it over and over again kinda thing .
D	I would've thought that umI think all the collections and things implement serializable already .
D	I think they might do .
D	Tonight I'll try and umI'll either work some more on uh the T_F_I_D_F_ summarizer or do the audio thing .
A	Don't know . Suppose we're just getting on with all our
D	Yeah .
A	components . So
D	Do we have to demonstrate something next week ?
A	I know .Wa
B	What do we have to demonstrate ?
A	Yeah . Yeah , he suggested that we could have an uh initial prototype .
A	I know , I'd b I'd be surprised if we can get anything working by next week .
D	Yeah .
A	Alright .
D	Yeah , I know .
D	I think it's 'cause we had to specify it ourselves that it's not as umlike focus the specification of most um work we have to do .
D	Yeah .
D	Once we start doing it it will all become more or less obvious I think anyway .
